{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa/mOjuXzvr4GlzY2MUp4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connecteev/langchain-crash-course-python-google-colab-notebook/blob/main/langchain_crash_course_python_google_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to LangChain:\n",
        "LangChain is a framework for developing applications powered by language models.According to their team the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\n",
        "\n",
        "- Be data-aware: connect a language model to other sources of data\n",
        "\n",
        "- Be agentic: allow a language model to interact with its environment\n",
        "\n",
        "**Links:**\n",
        "\n",
        "LangChain Docs: https://python.langchain.com/en/latest/index.html\n",
        "\n",
        "Github: https://github.com/hwchase17/langchain\n",
        "\n",
        "Based on LangChain Crash Course by PromptEngineering:\n",
        "https://www.youtube.com/watch?v=5-fc4Tlgmro"
      ],
      "metadata": {
        "id": "uJr_Q8wUa8qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topics to be covered:\n",
        "- Installation\n",
        "- Available LLMs\n",
        "- Prompt Templates\n",
        "- Chains\n",
        "- Agents & Tools\n",
        "- Memory\n",
        "- Document Loaders\n",
        "- Indexes"
      ],
      "metadata": {
        "id": "plLzOPNFbJzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "\n",
        "LangChain is available on PyPi, so to it is easily installable with:\n",
        "\n",
        "(ref: https://tinyurl.com/3fsppvxn)"
      ],
      "metadata": {
        "id": "c3cG7shickRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UFSnUwNcnQU",
        "outputId": "e28ee1c4-c991-4093-c892-4a0897b65813"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.229-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.229 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Available LLMs\n",
        "\n",
        "Has integration with several different LLMs, list is here: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"
      ],
      "metadata": {
        "id": "I4yo9rpXc2yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***OpenAI Integration***"
      ],
      "metadata": {
        "id": "7m6WW9JFc8Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XASDgTVhc-pe",
        "outputId": "ace3dfeb-7ec9-4aae-ee71-7ed9249435b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set your openai API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "G53rfhSXg2tE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
        "prompt = \"Why does the planet Saturn have rings around it?\"\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzEPcrmZhD1d",
        "outputId": "18efeb99-1c05-4909-905b-0a0827c9667b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The rings of Saturn are made up of particles of ice and rock ranging in size from micrometers to meters. Scientists believe that the rings of Saturn are mostly made up of particles that were once part of a larger object such as a comet or an asteroid. It is possible that these objects were pulled apart by Saturn’s gravity before eventually forming the rings. Some scientists also suggest that the rings of Saturn may have been formed by icy particles from the planet's moons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Huggingface Hub integration***"
      ],
      "metadata": {
        "id": "q5C0Rc3_hmVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBHfNDadhnhj",
        "outputId": "11ce65a9-fcf9-482a-abd2-7b8d3e22c9b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.6.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "0dQas0-zhsqN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "vpjC8RDzhuMG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_hub.html\n",
        "\n",
        "#llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.9, \"max_length\":64})\n",
        "#prompt = \"Why is gravity on the moon lower than that on earth?\"\n",
        "#print(llm(prompt))"
      ],
      "metadata": {
        "id": "2g4aAOg0h1V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates\n",
        "\n",
        "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.\n",
        "\n",
        "The prompt template may contain:\n",
        "\n",
        "- instructions to the language model,\n",
        "\n",
        "- a set of few shot examples to help the language model generate a better response,\n",
        "\n",
        "- a question to the language model.\n"
      ],
      "metadata": {
        "id": "R3WnGgguj8V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"Write a {adjective} poem about {subject}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"adjective\", \"subject\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "myPrompt = prompt.format(adjective='funny', subject='The Simpsons')\n",
        "\n",
        "myLLMOutput = llm(myPrompt)\n",
        "print(myLLMOutput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGg2pGaTj-mn",
        "outputId": "6b0f889f-ba2a-41e1-d644-a3b592d52fcb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Oh the Simpson family\n",
            "What a funny foursome they be \n",
            "With Homer always stuffing\n",
            "Frying up donuts so sweet\n",
            "\n",
            "Marge and her blue hair\n",
            "Battling her daily cares \n",
            "Lisa and her saxophone\n",
            "Keeping us in deep pensé \n",
            "\n",
            "Bart with his skateboard\n",
            "Always getting into trouble so hard \n",
            "And Maggie the baby\n",
            "Sucking her dummy so loud\n",
            "\n",
            "This family is something special\n",
            "The fans love them through it all \n",
            "But no matter what they do\n",
            "The Simpsons are here to stay for all.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The prompt template may contain:\n",
        "\n",
        "- instructions to the language model,\n",
        "\n",
        "- a set of few shot examples to help the language model generate a better response,\n",
        "\n",
        "- a question to the language model."
      ],
      "metadata": {
        "id": "qdVl5-m-m3pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "I want you to act as a naming consultant for new companies.\n",
        "\n",
        "Here are some examples of good company names:\n",
        "\n",
        "- search engine, Google\n",
        "- social media, Facebook\n",
        "- video sharing, YouTube\n",
        "\n",
        "The name should be short, catchy and easy to remember.\n",
        "\n",
        "What is a good name for a company that makes {product}?\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=template,\n",
        ")\n",
        "prompt = prompt.format(product='news articles')\n",
        "print(llm(prompt=prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y89mUHc5m0Yv",
        "outputId": "8958e9bb-163e-4bda-beef-36b23029d75e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NewsCircle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "\n",
        "Using an LLM in isolation is fine for some simple applications, but many more complex ones require chaining LLMs - either with each other or with other experts."
      ],
      "metadata": {
        "id": "pPu2rOTEnWkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "chainCommandOutput = chain.run(\"colorful socks\")\n",
        "print(chainCommandOutput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJw4dVZ-nZnA",
        "outputId": "afc57a60-0004-48de-89d2-a3403c9a025b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Vibrant Socksies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents & Tools\n",
        "\n",
        "Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n",
        "\n",
        "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "\n",
        "- LLM: The language model powering the agent.\n",
        "\n",
        "- Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "##### ***Potential use cases:***\n",
        "- Personal Assistant\n",
        "- Question Answering\n",
        "- Chatbots\n",
        "- Code Understanding etc.\n",
        "\n",
        "\n",
        "Tools: https://python.langchain.com/en/latest/modules/agents/tools.html\n",
        "\n",
        "Agents: https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kC4A9DOAn3SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFSOX7rxoDFq",
        "outputId": "53b976aa-7a1a-4ac2-bf29-746427dd83aa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.7)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "agent.run(\"What year did Lionel Messi Join Barcelona? What is his current age raised to the 0.43 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "YjWJknbUoSbM",
        "outputId": "27876243-a8f5-43d6-cd0a-088b73af9bd3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out when Messi joined Barcelona and compute his current age raised to the 0.43 power. \n",
            "Action: Wikipedia\n",
            "Action Input: Lionel Messi\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Lionel Messi\n",
            "Summary: Lionel Andrés Messi (Spanish pronunciation: [ljoˈnel anˈdɾes ˈmesi] (listen); born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward and captains the Argentina national team. Widely regarded as one of the greatest players of all time, Messi has won a record seven Ballon d'Or awards and a record six European Golden Shoes, and in 2020 he was named to the Ballon d'Or Dream Team. Until leaving the club in 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles and the UEFA Champions League four times. With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals in La Liga (474), most hat-tricks in La Liga (36) and the UEFA Champions League (eight), and most assists in La Liga (192) and the Copa América (17). He also has the most international goals by a South American male (103). Messi has scored over 800 senior career goals for club and country, and has the most goals by a player for a single club (672).\n",
            "Messi relocated to Spain from Argentina aged 13 to join Barcelona, for whom he made his competitive debut aged 17 in October 2004. He established himself as an integral player for the club within the next three years, and in his first uninterrupted season in 2008–09 he helped Barcelona achieve the first treble in Spanish football; that year, aged 22, Messi won his first Ballon d'Or. Three successful seasons followed, with Messi winning four consecutive Ballons d'Or, making him the first player to win the award four times. During the 2011–12 season, he set the La Liga and European records for most goals scored in a single season, while establishing himself as Barcelona's all-time top scorer. The following two seasons, Messi finished second for the Ballon d'Or behind Cristiano Ronaldo (his perceived career rival), before regaining his best form during the 2014–15 campaign, becoming the all-time top scorer in La Liga and leading Barcelona to a historic second treble, after which he was awarded a fifth Ballon d'Or in 2015. Messi assumed captaincy of Barcelona in 2018, and won a record sixth Ballon d'Or in 2019. Out of contract, he signed for French club Paris Saint-Germain in August 2021, spending two seasons at the club and winning Ligue 1 twice.\n",
            "An Argentine international, Messi is the country's all-time leading goalscorer and also holds the national record for appearances. At youth level, he won the 2005 FIFA World Youth Championship, finishing the tournament with both the Golden Ball and Golden Shoe, and an Olympic gold medal at the 2008 Summer Olympics. His style of play as a diminutive, left-footed dribbler drew comparisons with his compatriot Diego Maradona, who described Messi as his successor. After his senior debut in August 2005, Messi became the youngest Argentine to play and score in a FIFA World Cup (2006), and reached the final of the 2007 Copa América, where he was named young player of the tournament. As the squad's captain from August 2011, he led Argentina to three consecutive finals: the 2014 FIFA World Cup, for which he won the Golden Ball, the 2015 Copa América, winning the Golden Ball, and the 2016 Copa América. After announcing his international retirement in 2016, he reversed his decision and led his country to qualification for the 2018 FIFA World Cup, a third-place finish at the 2019 Copa América, and victory in the 2021 Copa América, while winning the Golden Ball and Golden Boot for the latter. For this achievement, Messi received a record-extending seventh Ballon d'Or in 2021. In 2022, he led Argentina to win the 2022 FIFA World Cup, where he won a record second Golden Ball, scored seven goals including two in the final, and broke the record for most games played at the World Cup (26).\n",
            "Messi has endorsed sportswear company \u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I have now found out when Messi joined Barcelona.\n",
            "Action: Calculator\n",
            "Action Input: 24 June 1987 raised to the 0.43 power\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.9218486893172186\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.9218486893172186.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lionel Messi joined Barcelona in 2004 and his current age raised to the 0.43 power is 3.9218486893172186.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "from langchain.tools import StructuredTool\n",
        "\n",
        "\n",
        "def multiplier(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply the provided floats.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "tool = StructuredTool.from_function(multiplier)\n",
        "\n",
        "# Structured tools are compatible with the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type.\n",
        "agent_executor = initialize_agent(\n",
        "    [tool],\n",
        "    llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "agent_executor.run(\"What is 3 times 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "RRJl6goQpC17",
        "outputId": "764fee93-24c7-4534-fe6e-3f738b464c54"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816d8e7580>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction:\n",
            "```\n",
            "{\n",
            "  \"action\": \"multiplier\",\n",
            "  \"action_input\": {\"a\": 3, \"b\": 4}\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m12.0\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816ce852a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I know what to respond\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Final Answer\",\n",
            "  \"action_input\": \"3 times 4 is 12.0\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3 times 4 is 12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Wikipedia tool integration, per https://python.langchain.com/docs/modules/agents/tools/integrations/wikipedia\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKwe_HCGpw4m",
        "outputId": "6bf1adb4-bae3-4b33-b5fc-6e4182074623"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "wikipedia.run(\"Man vs Wild\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "4H2Fn221s5ux",
        "outputId": "ed78d3ad-6b74-461b-aaa9-82c54384ed63"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816ceab460>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /tool-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816cea99f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: Man vs. Wild\\nSummary: Man vs. Wild, also called Born Survivor: Bear Grylls, Ultimate Survival, Survival Game, or colloquially as simply Bear Grylls in the United Kingdom, is a survival television series hosted by Bear Grylls on the Discovery Channel. In the United Kingdom, the series was originally shown on Channel 4, but the show\\'s later seasons were broadcast on Discovery Channel U.K. The series was produced by British television production company Diverse Bristol. The show was premiered on November 10, 2006, after airing a pilot episode titled \"The Rockies\" on March 10, 2006.\\nGrylls also said he has been approached about doing a Man vs. Wild urban disaster 3D feature film, which he said he would \"really like to do\". He signed on to showcase urban survival techniques in a Discovery show called Worst-Case Scenario, which premiered on May 5, 2010, on the network.The Discovery Channel terminated its legal relationship with Grylls in 2012 due to contract disputes, effectively canceling the series. In April 2019, Netflix brought Grylls back to the wilderness in the interactive series You vs. Wild, which includes eight episodes running approximately 20 minutes each.\\n\\nPage: List of Man vs. Wild episodes\\nSummary: Man vs. Wild is a television series on Discovery Channel in the United States, Australia, New Zealand, Canada, Brazil, India and Europe. The show is called Born Survivor in parts of Europe, including the UK, where it was originally broadcast by Channel 4, but latterly moved to Discovery Channel UK. In Africa, Asia and eastern parts of Europe, it is titled Ultimate Survival and also broadcast by Discovery Channel. \\n\\n\\n\\nPage: Man vs. Wild (video game)\\nSummary: Man vs. Wild  is a video game inspired by the Man vs. Wild survival television series. In it, the player takes on the role of host Bear Grylls to survive the hardships of various environments. It was developed by American companies Floor 84 Studio and Scientifically Proven.\\nThe game targeted all major seventh-generation platforms, but the handheld versions were ultimately cancelled.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory\n",
        "\n",
        "\n",
        "Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory."
      ],
      "metadata": {
        "id": "i3VdHYJetYSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "UMr53RFqtaGq",
        "outputId": "5dc4f824-2d82-4576-f003-de69a62e7aab"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816cea8ee0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816d8e7fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='Lets talk about how physics works on the Moon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "gs6SxkHKt0By",
        "outputId": "e9018e97-83a7-4ed4-8760-4b2f9e3f47b2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816cea9570>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: Lets talk about how physics works on the Moon\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816cea9750>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure! Physics on the Moon is a fascinating topic. The Moon's gravity is only about one-sixth of Earth's, so objects on the Moon experience a much weaker gravitational force. This means that objects on the Moon can move faster and farther than they can on Earth. Additionally, the Moon has no atmosphere, so there is no air resistance to slow down objects. This means that objects on the Moon can travel in a straight line for much longer distances than they can on Earth.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='Why is the gravitational field lower?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "BwVzVVCYt-XB",
        "outputId": "0d07d30f-a874-4ebd-c2b7-d15d8c6cfddb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816cea8c10>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: Lets talk about how physics works on the Moon\n",
            "AI:  Sure! Physics on the Moon is a fascinating topic. The Moon's gravity is only about one-sixth of Earth's, so objects on the Moon experience a much weaker gravitational force. This means that objects on the Moon can move faster and farther than they can on Earth. Additionally, the Moon has no atmosphere, so there is no air resistance to slow down objects. This means that objects on the Moon can travel in a straight line for much longer distances than they can on Earth.\n",
            "Human: Why is the gravitational field lower?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f816d8e68c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The Moon's gravitational field is lower because it is much smaller than Earth. The Moon has a mass of only 7.34767309 × 10^22 kg, compared to Earth's mass of 5.97219 × 10^24 kg. This means that the Moon's gravitational pull is much weaker than Earth's.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Loaders\n",
        "\n",
        "Combining language models with your own text data is a powerful way to differentiate them. The first step in doing this is to load the data into “documents” - a fancy way of say some pieces of text.\n",
        "\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders.html"
      ],
      "metadata": {
        "id": "qGsrHwe_uQMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOaL1cbFuPLy",
        "outputId": "75439ab1-3e51-4d80-ea8a-7d9c4a30a11e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.12.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "#loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
        "\n",
        "loader = PyPDFLoader(\"https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# print(pages[0])\n",
        "for page in pages:\n",
        "  print(page)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11eiScrmvZGh",
        "outputId": "45b24e2c-dc5d-4d55-c434-befcc400538c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Learning Word Vectors for Sentiment Analysis\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,\\nAndrew Y. Ng, and Christopher Potts\\nStanford University\\nStanford, CA 94305\\n[amaas, rdaly, ptpham, yuze, ang, cgpotts]@stanford.edu\\nAbstract\\nUnsupervised vector-based approaches to se-\\nmantics can model rich lexical meanings, but\\nthey largely fail to capture sentiment informa-\\ntion that is central to many word meanings and\\nimportant for a wide range of NLP tasks. We\\npresent a model that uses a mix of unsuper-\\nvised and supervised techniques to learn word\\nvectors capturing semantic term–document in-\\nformation as well as rich sentiment content.\\nThe proposed model can leverage both con-\\ntinuous and multi-dimensional sentiment in-\\nformation as well as non-sentiment annota-\\ntions. We instantiate the model to utilize the\\ndocument-level sentiment polarity annotations\\npresent in many online documents (e.g. star\\nratings). We evaluate the model using small,\\nwidely used sentiment and subjectivity cor-\\npora and ﬁnd it out-performs several previ-\\nously introduced methods for sentiment clas-\\nsiﬁcation. We also introduce a large dataset\\nof movie reviews to serve as a more robust\\nbenchmark for work in this area.\\n1 Introduction\\nWord representations are a critical component of\\nmany natural language processing systems. It is\\ncommon to represent words as indices in a vocab-\\nulary, but this fails to capture the rich relational\\nstructure of the lexicon. Vector-based models do\\nmuch better in this regard. They encode continu-\\nous similarities between words as distance or angle\\nbetween word vectors in a high-dimensional space.\\nThe general approach has proven useful in tasks\\nsuch as word sense disambiguation, named entityrecognition, part of speech tagging, and document\\nretrieval (Turney and Pantel, 2010; Collobert and\\nWeston, 2008; Turian et al., 2010).\\nIn this paper, we present a model to capture both\\nsemantic and sentiment similarities among words.\\nThe semantic component of our model learns word\\nvectors via an unsupervised probabilistic model of\\ndocuments. However, in keeping with linguistic and\\ncognitive research arguing that expressive content\\nand descriptive semantic content are distinct (Ka-\\nplan, 1999; Jay, 2000; Potts, 2007), we ﬁnd that\\nthis basic model misses crucial sentiment informa-\\ntion. For example, while it learns that wonderful\\nandamazing are semantically close, it doesn’t cap-\\nture the fact that these are both very strong positive\\nsentiment words, at the opposite end of the spectrum\\nfrom terrible andawful .\\nThus, we extend the model with a supervised\\nsentiment component that is capable of embracing\\nmany social and attitudinal aspects of meaning (Wil-\\nson et al., 2004; Alm et al., 2005; Andreevskaia\\nand Bergler, 2006; Pang and Lee, 2005; Goldberg\\nand Zhu, 2006; Snyder and Barzilay, 2007). This\\ncomponent of the model uses the vector represen-\\ntation of words to predict the sentiment annotations\\non contexts in which the words appear. This causes\\nwords expressing similar sentiment to have similar\\nvector representations. The full objective function\\nof the model thus learns semantic vectors that are\\nimbued with nuanced sentiment information. In our\\nexperiments, we show how the model can leverage\\ndocument-level sentiment annotations of a sort that\\nare abundant online in the form of consumer reviews\\nfor movies, products, etc. The technique is sufﬁ-' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 0}\n",
            "page_content='ciently general to work also with continuous and\\nmulti-dimensional notions of sentiment as well as\\nnon-sentiment annotations (e.g., political afﬁliation,\\nspeaker commitment).\\nAfter presenting the model in detail, we pro-\\nvide illustrative examples of the vectors it learns,\\nand then we systematically evaluate the approach\\non document-level and sentence-level classiﬁcation\\ntasks. Our experiments involve the small, widely\\nused sentiment and subjectivity corpora of Pang and\\nLee (2004), which permits us to make comparisons\\nwith a number of related approaches and published\\nresults. We also show that this dataset contains many\\ncorrelations between examples in the training and\\ntesting sets. This leads us to evaluate on, and make\\npublicly available, a large dataset of informal movie\\nreviews from the Internet Movie Database (IMDB).\\n2 Related work\\nThe model we present in the next section draws in-\\nspiration from prior work on both probabilistic topic\\nmodeling and vector-spaced models for word mean-\\nings.\\nLatent Dirichlet Allocation (LDA; (Blei et al.,\\n2003)) is a probabilistic document model that as-\\nsumes each document is a mixture of latent top-\\nics. For each latent topic T, the model learns a\\nconditional distribution p(wjT)for the probability\\nthat wordwoccurs inT. One can obtain a k-\\ndimensional vector representation of words by ﬁrst\\ntraining ak-topic model and then ﬁlling the matrix\\nwith thep(wjT)values (normalized to unit length).\\nThe result is a word–topic matrix in which the rows\\nare taken to represent word meanings. However,\\nbecause the emphasis in LDA is on modeling top-\\nics, not word meanings, there is no guarantee that\\nthe row (word) vectors are sensible as points in a\\nk-dimensional space. Indeed, we show in section\\n4 that using LDA in this way does not deliver ro-\\nbust word vectors. The semantic component of our\\nmodel shares its probabilistic foundation with LDA,\\nbut is factored in a manner designed to discover\\nword vectors rather than latent topics. Some recent\\nwork introduces extensions of LDA to capture sen-\\ntiment in addition to topical information (Li et al.,\\n2010; Lin and He, 2009; Boyd-Graber and Resnik,\\n2010). Like LDA, these methods focus on model-ing sentiment-imbued topics rather than embedding\\nwords in a vector space.\\nVector space models (VSMs) seek to model words\\ndirectly (Turney and Pantel, 2010). Latent Seman-\\ntic Analysis (LSA), perhaps the best known VSM,\\nexplicitly learns semantic word vectors by apply-\\ning singular value decomposition (SVD) to factor a\\nterm–document co-occurrence matrix. It is typical\\nto weight and normalize the matrix values prior to\\nSVD. To obtain a k-dimensional representation for a\\ngiven word, only the entries corresponding to the k\\nlargest singular values are taken from the word’s ba-\\nsis in the factored matrix. Such matrix factorization-\\nbased approaches are extremely successful in prac-\\ntice, but they force the researcher to make a number\\nof design choices (weighting, normalization, dimen-\\nsionality reduction algorithm) with little theoretical\\nguidance to suggest which to prefer.\\nUsing term frequency (tf) and inverse document\\nfrequency (idf) weighting to transform the values\\nin a VSM often increases the performance of re-\\ntrieval and categorization systems. Delta idf weight-\\ning (Martineau and Finin, 2009) is a supervised vari-\\nant of idf weighting in which the idf calculation is\\ndone for each document class and then one value\\nis subtracted from the other. Martineau and Finin\\npresent evidence that this weighting helps with sen-\\ntiment classiﬁcation, and Paltoglou and Thelwall\\n(2010) systematically explore a number of weight-\\ning schemes in the context of sentiment analysis.\\nThe success of delta idf weighting in previous work\\nsuggests that incorporating sentiment information\\ninto VSM values via supervised methods is help-\\nful for sentiment analysis. We adopt this insight,\\nbut we are able to incorporate it directly into our\\nmodel’s objective function. (Section 4 compares' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 1}\n",
            "page_content='into VSM values via supervised methods is help-\\nful for sentiment analysis. We adopt this insight,\\nbut we are able to incorporate it directly into our\\nmodel’s objective function. (Section 4 compares\\nour approach with a representative sample of such\\nweighting schemes.)\\n3 Our Model\\nTo capture semantic similarities among words, we\\nderive a probabilistic model of documents which\\nlearns word representations. This component does\\nnot require labeled data, and shares its foundation\\nwith probabilistic topic models such as LDA. The\\nsentiment component of our model uses sentiment\\nannotations to constrain words expressing similar' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 1}\n",
            "page_content='sentiment to have similar representations. We can\\nefﬁciently learn parameters for the joint objective\\nfunction using alternating maximization.\\n3.1 Capturing Semantic Similarities\\nWe build a probabilistic model of a document us-\\ning a continuous mixture distribution over words in-\\ndexed by a multi-dimensional random variable \\x12.\\nWe assume words in a document are conditionally\\nindependent given the mixture variable \\x12. We assign\\na probability to a document dusing a joint distribu-\\ntion over the document and \\x12. The model assumes\\neach wordwi2dis conditionally independent of\\nthe other words given \\x12. The probability of a docu-\\nment is thus\\np(d) =Z\\np(d;\\x12)d\\x12=Z\\np(\\x12)NY\\ni=1p(wij\\x12)d\\x12: (1)\\nWhereNis the number of words in dandwiis\\ntheithword ind. We use a Gaussian prior on \\x12.\\nWe deﬁne the conditional distribution p(wij\\x12)us-\\ning a log-linear model with parameters Randb.\\nThe energy function uses a word representation ma-\\ntrixR2R(\\x0cxjVj)where each word w(represented\\nas a one-on vector) in the vocabulary Vhas a\\x0c-\\ndimensional vector representation \\x1ew=Rwcorre-\\nsponding to that word’s column in R. The random\\nvariable\\x12is also a\\x0c-dimensional vector, \\x122R\\x0c\\nwhich weights each of the \\x0cdimensions of words’\\nrepresentation vectors. We additionally introduce a\\nbiasbwfor each word to capture differences in over-\\nall word frequencies. The energy assigned to a word\\nwgiven these model parameters is\\nE(w;\\x12;\\x1ew;bw) =\\x00\\x12T\\x1ew\\x00bw: (2)\\nTo obtain the distribution p(wj\\x12)we use a softmax,\\np(wj\\x12;R;b) =exp(\\x00E(w;\\x12;\\x1ew;bw))P\\nw02Vexp(\\x00E(w0;\\x12;\\x1ew0;bw0))\\n(3)\\n=exp(\\x12T\\x1ew+bw)P\\nw02Vexp(\\x12T\\x1ew0+bw0): (4)\\nThe number of terms in the denominator’s sum-\\nmation grows linearly in jVj, making exact com-\\nputation of the distribution possible. For a given\\n\\x12, a wordw’s occurrence probability is related tohow closely its representation vector \\x1ewmatches the\\nscaling direction of \\x12. This idea is similar to the\\nword vector inner product used in the log-bilinear\\nlanguage model of Mnih and Hinton (2007).\\nEquation 1 resembles the probabilistic model of\\nLDA (Blei et al., 2003), which models documents\\nas mixtures of latent topics. One could view the en-\\ntries of a word vector \\x1eas that word’s association\\nstrength with respect to each latent topic dimension.\\nThe random variable \\x12then deﬁnes a weighting over\\ntopics. However, our model does not attempt to\\nmodel individual topics, but instead directly models\\nword probabilities conditioned on the topic mixture\\nvariable\\x12. Because of the log-linear formulation of\\nthe conditional distribution, \\x12is a vector in R\\x0cand\\nnot restricted to the unit simplex as it is in LDA.\\nWe now derive maximum likelihood learning for\\nthis model when given a set of unlabeled documents\\nD. In maximum likelihood learning we maximize\\nthe probability of the observed data given the model\\nparameters. We assume documents dk2Dare i.i.d.\\nsamples. Thus the learning problem becomes\\nmax\\nR;bp(D;R;b) =Y\\ndk2DZ\\np(\\x12)NkY\\ni=1p(wij\\x12;R;b)d\\x12:\\n(5)\\nUsing maximum a posteriori (MAP) estimates for \\x12,\\nwe approximate this learning problem as\\nmax\\nR;bY\\ndk2Dp(^\\x12k)NkY\\ni=1p(wij^\\x12k;R;b); (6)\\nwhere ^\\x12kdenotes the MAP estimate of \\x12fordk.\\nWe introduce a Frobenious norm regularization term\\nfor the word representation matrix R. The word bi-\\nasesbare not regularized reﬂecting the fact that we\\nwant the biases to capture whatever overall word fre-\\nquency statistics are present in the data. By taking\\nthe logarithm and simplifying we obtain the ﬁnal ob-\\njective,\\n\\x17jjRjj2\\nF+X\\ndk2D\\x15jj^\\x12kjj2\\n2+NkX\\ni=1logp(wij^\\x12k;R;b);\\n(7)\\nwhich is maximized with respect to Randb. The\\nhyper-parameters in the model are the regularization' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 2}\n",
            "page_content='weights (\\x15and\\x17), and the word vector dimension-\\nality\\x0c.\\n3.2 Capturing Word Sentiment\\nThe model presented so far does not explicitly cap-\\nture sentiment information. Applying this algorithm\\nto documents will produce representations where\\nwords that occur together in documents have sim-\\nilar representations. However, this unsupervised\\napproach has no explicit way of capturing which\\nwords are predictive of sentiment as opposed to\\ncontent-related. Much previous work in natural lan-\\nguage processing achieves better representations by\\nlearning from multiple tasks (Collobert and Weston,\\n2008; Finkel and Manning, 2009). Following this\\ntheme we introduce a second task to utilize labeled\\ndocuments to improve our model’s word representa-\\ntions.\\nSentiment is a complex, multi-dimensional con-\\ncept. Depending on which aspects of sentiment we\\nwish to capture, we can give some body of text a\\nsentiment label swhich can be categorical, continu-\\nous, or multi-dimensional. To leverage such labels,\\nwe introduce an objective that the word vectors of\\nour model should predict the sentiment label using\\nsome appropriate predictor,\\n^s=f(\\x1ew): (8)\\nUsing an appropriate predictor function f(x)we\\nmap a word vector \\x1ewto a predicted sentiment label\\n^s. We can then improve our word vector \\x1ewto better\\npredict the sentiment labels of contexts in which that\\nword occurs.\\nFor simplicity we consider the case where the sen-\\ntiment label sis a scalar continuous value repre-\\nsenting sentiment polarity of a document. This cap-\\ntures the case of many online reviews where doc-\\numents are associated with a label on a star rating\\nscale. We linearly map such star values to the inter-\\nvals2[0;1]and treat them as a probability of pos-\\nitive sentiment polarity. Using this formulation, we\\nemploy a logistic regression as our predictor f(x).\\nWe usew’s vector representation \\x1ewand regression\\nweights to express this as\\np(s= 1jw;R; ) =\\x1b( T\\x1ew+bc); (9)where\\x1b(x)is the logistic function and  2R\\x0cis the\\nlogistic regression weight vector. We additionally\\nintroduce a scalar bias bcfor the classiﬁer.\\nThe logistic regression weights  andbcdeﬁne\\na linear hyperplane in the word vector space where\\na word vector’s positive sentiment probability de-\\npends on where it lies with respect to this hyper-\\nplane. Learning over a collection of documents re-\\nsults in words residing different distances from this\\nhyperplane based on the average polarity of docu-\\nments in which the words occur.\\nGiven a set of labeled documents Dwhereskis\\nthe sentiment label for document dk, we wish to\\nmaximize the probability of document labels given\\nthe documents. We assume documents in the collec-\\ntion and words within a document are i.i.d. samples.\\nBy maximizing the log-objective we obtain,\\nmax\\nR; ;b cjDjX\\nk=1NkX\\ni=1logp(skjwi;R; ;bc): (10)\\nThe conditional probability p(skjwi;R; ;bc)is\\neasily obtained from equation 9.\\n3.3 Learning\\nThe full learning objective maximizes a sum of the\\ntwo objectives presented. This produces a ﬁnal ob-\\njective function of,\\n\\x17jjRjj2\\nF+jDjX\\nk=1\\x15jj^\\x12kjj2\\n2+NkX\\ni=1logp(wij^\\x12k;R;b)\\n+jDjX\\nk=11\\njSkjNkX\\ni=1logp(skjwi;R; ;bc): (11)\\njSkjdenotes the number of documents in the dataset\\nwith the same rounded value of sk(i.e.sk<0:5\\nandsk\\x150:5). We introduce the weighting1\\njSkjto\\ncombat the well-known imbalance in ratings present\\nin review collections. This weighting prevents the\\noverall distribution of document ratings from affect-\\ning the estimate of document ratings in which a par-\\nticular word occurs. The hyper-parameters of the\\nmodel are the regularization weights ( \\x15and\\x17), and\\nthe word vector dimensionality \\x0c.\\nMaximizing the objective function with respect to\\nR,b, , andbcis a non-convex problem. We use\\nalternating maximization, which ﬁrst optimizes the' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 3}\n",
            "page_content='word representations ( R,b, , andbc) while leav-\\ning the MAP estimates ( ^\\x12) ﬁxed. Then we ﬁnd the\\nnew MAP estimate for each document while leav-\\ning the word representations ﬁxed, and continue this\\nprocess until convergence. The optimization algo-\\nrithm quickly ﬁnds a global solution for each ^\\x12kbe-\\ncause we have a low-dimensional, convex problem\\nin each ^\\x12k. Because the MAP estimation problems\\nfor different documents are independent, we can\\nsolve them on separate machines in parallel. This\\nfacilitates scaling the model to document collections\\nwith hundreds of thousands of documents.\\n4 Experiments\\nWe evaluate our model with document-level and\\nsentence-level categorization tasks in the domain of\\nonline movie reviews. For document categoriza-\\ntion, we compare our method to previously pub-\\nlished results on a standard dataset, and introduce\\na new dataset for the task. In both tasks we com-\\npare our model’s word representations with several\\nbag of words weighting methods, and alternative ap-\\nproaches to word vector induction.\\n4.1 Word Representation Learning\\nWe induce word representations with our model us-\\ning 25,000 movie reviews from IMDB. Because\\nsome movies receive substantially more reviews\\nthan others, we limited ourselves to including at\\nmost 30 reviews from any movie in the collection.\\nWe build a ﬁxed dictionary of the 5,000 most fre-\\nquent tokens, but ignore the 50 most frequent terms\\nfrom the original full vocabulary. Traditional stop\\nword removal was not used because certain stop\\nwords (e.g. negating words) are indicative of senti-\\nment. Stemming was not applied because the model\\nlearns similar representations for words of the same\\nstem when the data suggests it. Additionally, be-\\ncause certain non-word tokens (e.g. “!” and “:-)” )\\nare indicative of sentiment, we allow them in our vo-\\ncabulary. Ratings on IMDB are given as star values\\n(2f1;2;:::;10g), which we linearly map to [0;1]to\\nuse as document labels when training our model.\\nThe semantic component of our model does not\\nrequire document labels. We train a variant of our\\nmodel which uses 50,000 unlabeled reviews in addi-\\ntion to the labeled set of 25,000 reviews. The unla-beled set of reviews contains neutral reviews as well\\nas those which are polarized as found in the labeled\\nset. Training the model with additional unlabeled\\ndata captures a common scenario where the amount\\nof labeled data is small relative to the amount of un-\\nlabeled data available. For all word vector models,\\nwe use 50-dimensional vectors.\\nAs a qualitative assessment of word represen-\\ntations, we visualize the words most similar to a\\nquery word using vector similarity of the learned\\nrepresentations. Given a query word wand an-\\nother wordw0we obtain their vector representations\\n\\x1ewand\\x1ew0, and evaluate their cosine similarity as\\nS(\\x1ew;\\x1ew0) =\\x1eT\\nw\\x1ew0\\njj\\x1ewjj\\x01jj\\x1ew0jj. By assessing the simi-\\nlarity ofwwith all other words w0, we can ﬁnd the\\nwords deemed most similar by the model.\\nTable 1 shows the most similar words to given\\nquery words using our model’s word representations\\nas well as those of LSA. All of these vectors cap-\\nture broad semantic similarities. However, both ver-\\nsions of our model seem to do better than LSA in\\navoiding accidental distributional similarities (e.g.,\\nscrewball andgrant as similar to romantic ) A com-\\nparison of the two versions of our model also begins\\nto highlight the importance of adding sentiment in-\\nformation. In general, words indicative of sentiment\\ntend to have high similarity with words of the same\\nsentiment polarity, so even the purely unsupervised\\nmodel’s results look promising. However, they also\\nshow more genre and content effects. For exam-\\nple, the sentiment enriched vectors for ghastly are\\ntruly semantic alternatives to that word, whereas the\\nvectors without sentiment also contain some content\\nwords that tend to have ghastly predicated of them.\\nOf course, this is only an impressionistic analysis of\\na few cases, but it is helpful in understanding why' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 4}\n",
            "page_content='words that tend to have ghastly predicated of them.\\nOf course, this is only an impressionistic analysis of\\na few cases, but it is helpful in understanding why\\nthe sentiment-enriched model proves superior at the\\nsentiment classiﬁcation results we report next.\\n4.2 Other Word Representations\\nFor comparison, we implemented several alternative\\nvector space models that are conceptually similar to\\nour own, as discussed in section 2:\\nLatent Semantic Analysis (LSA; Deerwester et\\nal., 1990) We apply truncated SVD to a tf.idf\\nweighted, cosine normalized count matrix, which\\nis a standard weighting and smoothing scheme for' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 4}\n",
            "page_content='Our model Our model\\nSentiment + Semantic Semantic only LSA\\nmelancholybittersweet thoughtful poetic\\nheartbreaking warmth lyrical\\nhappiness layer poetry\\ntenderness gentle profound\\ncompassionate loneliness vivid\\nghastlyembarrassingly predators hideous\\ntrite hideous inept\\nlaughably tube severely\\natrocious bafﬂed grotesque\\nappalling smack unsuspecting\\nlacklusterlame passable uninspired\\nlaughable unconvincing ﬂat\\nunimaginative amateurish bland\\nuninspired clich ´ed forgettable\\nawful insipid mediocre\\nromanticromance romance romance\\nlove charming screwball\\nsweet delightful grant\\nbeautiful sweet comedies\\nrelationship chemistry comedy\\nTable 1: Similarity of learned word vectors. Each target word is given with its ﬁve most similar words using cosine\\nsimilarity of the vectors determined by each model. The full version of our model (left) captures both lexical similarity\\nas well as similarity of sentiment strength and orientation. Our unsupervised semantic component (center) and LSA\\n(right) capture semantic relations.\\nVSM induction (Turney and Pantel, 2010).\\nLatent Dirichlet Allocation (LDA; Blei et\\nal., 2003) We use the method described in sec-\\ntion 2 for inducing word representations from the\\ntopic matrix. To train the 50-topic LDA model we\\nuse code released by Blei et al. (2003). We use the\\nsame 5,000 term vocabulary for LDA as is used for\\ntraining word vector models. We leave the LDA\\nhyperparameters at their default values, though\\nsome work suggests optimizing over priors for LDA\\nis important (Wallach et al., 2009).\\nWeighting Variants We evaluate both binary (b)\\nterm frequency weighting with smoothed delta idf\\n(\\x01t’) and no idf (n) because these variants worked\\nwell in previous experiments in sentiment (Mar-\\ntineau and Finin, 2009; Pang et al., 2002). In all\\ncases, we use cosine normalization (c). Paltoglou\\nand Thelwall (2010) perform an extensive analysisof such weighting variants for sentiment tasks.\\n4.3 Document Polarity Classiﬁcation\\nOur ﬁrst evaluation task is document-level senti-\\nment polarity classiﬁcation. A classiﬁer must pre-\\ndict whether a given review is positive or negative\\ngiven the review text.\\nGiven a document’s bag of words vector v, we\\nobtain features from our model using a matrix-\\nvector product Rv, wherevcan have arbitrary tf.idf\\nweighting. We do not cosine normalize v, instead\\napplying cosine normalization to the ﬁnal feature\\nvectorRv. This procedure is also used to obtain\\nfeatures from the LDA and LSA word vectors. In\\npreliminary experiments, we found ‘bnn’ weighting\\nto work best for vwhen generating document fea-\\ntures via the product Rv. In all experiments, we\\nuse this weighting to get multi-word representations' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 5}\n",
            "page_content='Features PL04 Our Dataset Subjectivity\\nBag of Words (bnc) 85.45 87.80 87.77\\nBag of Words (b \\x01t’c) 85.80 88.23 85.65\\nLDA 66.70 67.42 66.65\\nLSA 84.55 83.96 82.82\\nOur Semantic Only 87.10 87.30 86.65\\nOur Full 84.65 87.44 86.19\\nOur Full, Additional Unlabeled 87.05 87.99 87.22\\nOur Semantic + Bag of Words (bnc) 88.30 88.28 88.58\\nOur Full + Bag of Words (bnc) 87.85 88.33 88.45\\nOur Full, Add’l Unlabeled + Bag of Words (bnc) 88.90 88.89 88.13\\nBag of Words SVM (Pang and Lee, 2004) 87.15 N/A 90.00\\nContextual Valence Shifters (Kennedy and Inkpen, 2006) 86.20 N/A N/A\\ntf.\\x01idf Weighting (Martineau and Finin, 2009) 88.10 N/A N/A\\nAppraisal Taxonomy (Whitelaw et al., 2005) 90.20 N/A N/A\\nTable 2: Classiﬁcation accuracy on three tasks. From left to right the datasets are: A collection of 2,000 movie reviews\\noften used as a benchmark of sentiment classiﬁcation (Pang and Lee, 2004), 50,000 reviews we gathered from IMDB,\\nand the sentence subjectivity dataset also released by (Pang and Lee, 2004). All tasks are balanced two-class problems.\\nfrom word vectors.\\n4.3.1 Pang and Lee Movie Review Dataset\\nThe polarity dataset version 2.0 introduced\\nby Pang and Lee (2004)1consists of 2,000 movie\\nreviews, where each is associated with a binary sen-\\ntiment polarity label. We report 10-fold cross vali-\\ndation results using the authors’ published folds to\\nmake our results comparable with others in the lit-\\nerature. We use a linear support vector machine\\n(SVM) classiﬁer trained with LIBLINEAR (Fan et\\nal., 2008), and set the SVM regularization parame-\\nter to the same value used by Pang and Lee (2004).\\nTable 2 shows the classiﬁcation performance of\\nour method, other VSMs we implemented, and pre-\\nviously reported results from the literature. Bag of\\nwords vectors are denoted by their weighting nota-\\ntion. Features from word vector learner are denoted\\nby the learner name. As a control, we trained ver-\\nsions of our model with only the unsupervised se-\\nmantic component, and the full model (semantic and\\nsentiment). We also include results for a version of\\nour full model trained with 50,000 additional unla-\\nbeled examples. Finally, to test whether our mod-\\nels’ representations complement a standard bag of\\nwords, we evaluate performance of the two feature\\nrepresentations concatenated.\\n1http://www.cs.cornell.edu/people/pabo/movie-review-dataOur method’s features clearly outperform those of\\nother VSMs, and perform best when combined with\\nthe original bag of words representation. The vari-\\nant of our model trained with additional unlabeled\\ndata performed best, suggesting the model can effec-\\ntively utilize large amounts of unlabeled data along\\nwith labeled examples. Our method performs com-\\npetitively with previously reported results in spite of\\nour restriction to a vocabulary of only 5,000 words.\\nWe extracted the movie title associated with each\\nreview and found that 1,299 of the 2,000 reviews in\\nthe dataset have at least one other review of the same\\nmovie in the dataset. Of 406 movies with multiple\\nreviews, 249 have the same polarity label for all of\\ntheir reviews. Overall, these facts suggest that, rela-\\ntive to the size of the dataset, there are highly corre-\\nlated examples with correlated labels. This is a nat-\\nural and expected property of this kind of document\\ncollection, but it can have a substantial impact on\\nperformance in datasets of this scale. In the random\\nfolds distributed by the authors, approximately 50%\\nof reviews in each validation fold’s test set have a\\nreview of the same movie with the same label in the\\ntraining set. Because the dataset is small, a learner\\nmay perform well by memorizing the association be-\\ntween label and words unique to a particular movie\\n(e.g., character names or plot terms).\\nWe introduce a substantially larger dataset, which' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 6}\n",
            "page_content='uses disjoint sets of movies for training and testing.\\nThese steps minimize the ability of a learner to rely\\non idiosyncratic word–class associations, thereby\\nfocusing attention on genuine sentiment features.\\n4.3.2 IMDB Review Dataset\\nWe constructed a collection of 50,000 reviews\\nfrom IMDB, allowing no more than 30 reviews per\\nmovie. The constructed dataset contains an even\\nnumber of positive and negative reviews, so ran-\\ndomly guessing yields 50% accuracy. Following\\nprevious work on polarity classiﬁcation, we consider\\nonly highly polarized reviews. A negative review\\nhas a score\\x144out of 10, and a positive review has\\na score\\x157out of 10. Neutral reviews are not in-\\ncluded in the dataset. In the interest of providing a\\nbenchmark for future work in this area, we release\\nthis dataset to the public.2\\nWe evenly divided the dataset into training and\\ntest sets. The training set is the same 25,000 la-\\nbeled reviews used to induce word vectors with our\\nmodel. We evaluate classiﬁer performance after\\ncross-validating classiﬁer parameters on the training\\nset, again using a linear SVM in all cases. Table 2\\nshows classiﬁcation performance on our subset of\\nIMDB reviews. Our model showed superior per-\\nformance to other approaches, and performed best\\nwhen concatenated with bag of words representa-\\ntion. Again the variant of our model which utilized\\nextra unlabeled data during training performed best.\\nDifferences in accuracy are small, but, because\\nour test set contains 25,000 examples, the variance\\nof the performance estimate is quite low. For ex-\\nample, an accuracy increase of 0.1% corresponds to\\ncorrectly classifying an additional 25 reviews.\\n4.4 Subjectivity Detection\\nAs a second evaluation task, we performed sentence-\\nlevel subjectivity classiﬁcation. In this task, a clas-\\nsiﬁer is trained to decide whether a given sentence is\\nsubjective, expressing the writer’s opinions, or ob-\\njective, expressing purely facts. We used the dataset\\nof Pang and Lee (2004), which contains subjective\\nsentences from movie review summaries and objec-\\ntive sentences from movie plot summaries. This task\\n2Dataset and further details are available online at:\\nhttp://www.andrew-maas.net/data/sentimentis substantially different from the review classiﬁca-\\ntion task because it uses sentences as opposed to en-\\ntire documents and the target concept is subjectivity\\ninstead of opinion polarity. We randomly split the\\n10,000 examples into 10 folds and report 10-fold\\ncross validation accuracy using the SVM training\\nprotocol of Pang and Lee (2004).\\nTable 2 shows classiﬁcation accuracies from the\\nsentence subjectivity experiment. Our model again\\nprovided superior features when compared against\\nother VSMs. Improvement over the bag-of-words\\nbaseline is obtained by concatenating the two feature\\nvectors.\\n5 Discussion\\nWe presented a vector space model that learns word\\nrepresentations captuing semantic and sentiment in-\\nformation. The model’s probabilistic foundation\\ngives a theoretically justiﬁed technique for word\\nvector induction as an alternative to the overwhelm-\\ning number of matrix factorization-based techniques\\ncommonly used. Our model is parametrized as a\\nlog-bilinear model following recent success in us-\\ning similar techniques for language models (Bengio\\net al., 2003; Collobert and Weston, 2008; Mnih and\\nHinton, 2007), and it is related to probabilistic latent\\ntopic models (Blei et al., 2003; Steyvers and Grif-\\nﬁths, 2006). We parametrize the topical component\\nof our model in a manner that aims to capture word\\nrepresentations instead of latent topics. In our ex-\\nperiments, our method performed better than LDA,\\nwhich models latent topics directly.\\nWe extended the unsupervised model to incor-\\nporate sentiment information and showed how this\\nextended model can leverage the abundance of\\nsentiment-labeled texts available online to yield\\nword representations that capture both sentiment\\nand semantic relations. We demonstrated the util-' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 7}\n",
            "page_content='extended model can leverage the abundance of\\nsentiment-labeled texts available online to yield\\nword representations that capture both sentiment\\nand semantic relations. We demonstrated the util-\\nity of such representations on two tasks of senti-\\nment classiﬁcation, using existing datasets as well\\nas a larger one that we release for future research.\\nThese tasks involve relatively simple sentiment in-\\nformation, but the model is highly ﬂexible in this\\nregard; it can be used to characterize a wide variety\\nof annotations, and thus is broadly applicable in the\\ngrowing areas of sentiment analysis and retrieval.' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 7}\n",
            "page_content='References\\nC. O. Alm, D. Roth, and R. Sproat. 2005. Emotions from\\ntext: machine learning for text-based emotion predic-\\ntion. In Proceedings of HLT/EMNLP , pages 579–586.\\nA. Andreevskaia and S. Bergler. 2006. Mining Word-\\nNet for fuzzy sentiment: sentiment tag extraction from\\nWordNet glosses. In Proceedings of the European\\nACL, pages 209–216.\\nY . Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\na neural probabilistic language model. Journal of Ma-\\nchine Learning Research , 3:1137–1155, August.\\nD. M. Blei, A. Y . Ng, and M. I. Jordan. 2003. Latent\\ndirichlet allocation. Journal of Machine Learning Re-\\nsearch , 3:993–1022, May.\\nJ. Boyd-Graber and P. Resnik. 2010. Holistic sentiment\\nanalysis across languages: multilingual supervised la-\\ntent Dirichlet allocation. In Proceedings of EMNLP ,\\npages 45–55.\\nR. Collobert and J. Weston. 2008. A uniﬁed architecture\\nfor natural language processing. In Proceedings of the\\nICML , pages 160–167.\\nS. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-\\ndauer, and R. Harshman. 1990. Indexing by latent se-\\nmantic analysis. Journal of the American Society for\\nInformation Science , 41:391–407, September.\\nR. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and\\nC. J. Lin. 2008. LIBLINEAR: A library for large lin-\\near classiﬁcation. The Journal of Machine Learning\\nResearch , 9:1871–1874, August.\\nJ. R. Finkel and C. D. Manning. 2009. Joint parsing and\\nnamed entity recognition. In Proceedings of NAACL ,\\npages 326–334.\\nA. B. Goldberg and J. Zhu. 2006. Seeing stars when\\nthere aren’t many stars: graph-based semi-supervised\\nlearning for sentiment categorization. In TextGraphs:\\nHLT/NAACL Workshop on Graph-based Algorithms\\nfor Natural Language Processing , pages 45–52.\\nT. Jay. 2000. Why We Curse: A Neuro-Psycho-\\nSocial Theory of Speech . John Benjamins, Philadel-\\nphia/Amsterdam.\\nD. Kaplan. 1999. What is meaning? Explorations in the\\ntheory of Meaning as Use . Brief version — draft 1.\\nMs., UCLA.\\nA. Kennedy and D. Inkpen. 2006. Sentiment clas-\\nsiﬁcation of movie reviews using contextual valence\\nshifters. Computational Intelligence , 22:110–125,\\nMay.\\nF. Li, M. Huang, and X. Zhu. 2010. Sentiment analysis\\nwith global topics and local dependency. In Proceed-\\nings of AAAI , pages 1371–1376.\\nC. Lin and Y . He. 2009. Joint sentiment/topic model for\\nsentiment analysis. In Proceeding of the 18th ACMConference on Information and Knowledge Manage-\\nment , pages 375–384.\\nJ. Martineau and T. Finin. 2009. Delta tﬁdf: an improved\\nfeature space for sentiment analysis. In Proceedings\\nof the 3rd AAAI International Conference on Weblogs\\nand Social Media , pages 258–261.\\nA. Mnih and G. E. Hinton. 2007. Three new graphical\\nmodels for statistical language modelling. In Proceed-\\nings of the ICML , pages 641–648.\\nG. Paltoglou and M. Thelwall. 2010. A study of informa-\\ntion retrieval weighting schemes for sentiment analy-\\nsis. In Proceedings of the ACL , pages 1386–1395.\\nB. Pang and L. Lee. 2004. A sentimental education:\\nsentiment analysis using subjectivity summarization\\nbased on minimum cuts. In Proceedings of the ACL ,\\npages 271–278.\\nB. Pang and L. Lee. 2005. Seeing stars: exploiting class\\nrelationships for sentiment categorization with respect\\nto rating scales. In Proceedings of ACL , pages 115–\\n124.\\nB. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs\\nup? sentiment classiﬁcation using machine learning\\ntechniques. In Proceedings of EMNLP , pages 79–86.\\nC. Potts. 2007. The expressive dimension. Theoretical\\nLinguistics , 33:165–197.\\nB. Snyder and R. Barzilay. 2007. Multiple aspect rank-\\ning using the good grief algorithm. In Proceedings of\\nthe Joint Human Language Technology/North Amer-\\nican Chapter of the ACL Conference (HLT-NAACL) ,\\npages 300–307.\\nM. Steyvers and T. L. Grifﬁths. 2006. Probabilistic topic\\nmodels. In T. Landauer, D McNamara, S. Dennis, and\\nW. Kintsch, editors, Latent Semantic Analysis: A Road\\nto Meaning .\\nJ. Turian, L. Ratinov, and Y . Bengio. 2010. Word rep-\\nresentations: A simple and general method for semi-' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 8}\n",
            "page_content='W. Kintsch, editors, Latent Semantic Analysis: A Road\\nto Meaning .\\nJ. Turian, L. Ratinov, and Y . Bengio. 2010. Word rep-\\nresentations: A simple and general method for semi-\\nsupervised learning. In Proceedings of the ACL , page\\n384394.\\nP. D. Turney and P. Pantel. 2010. From frequency to\\nmeaning: vector space models of semantics. Journal\\nof Artiﬁcial Intelligence Research , 37:141–188.\\nH. Wallach, D. Mimno, and A. McCallum. 2009. Re-\\nthinking LDA: why priors matter. In Proceedings of\\nNIPS , pages 1973–1981.\\nC. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-\\npraisal groups for sentiment analysis. In Proceedings\\nof CIKM , pages 625–631.\\nT. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad\\nare you? Finding strong and weak opinion clauses. In\\nProceedings of AAAI , pages 761–769.' metadata={'source': '/tmp/tmp7ttrtlre/tmp.pdf', 'page': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexes\n",
        "\n",
        "Indexes refer to ways to structure documents so that LLMs can best interact with them. This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.\n",
        "\n",
        "- Embeddings: An embedding is a numerical representation of a piece of information, for example, text, documents, images, audio, etc.\n",
        "- Text Splitters: When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\n",
        "- Vectorstores: Vector databases store and index vector embeddings from NLP models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results.\n"
      ],
      "metadata": {
        "id": "I7IAzGGbzH0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/langchain/master/docs/extras/modules/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "4NVTikLVzJ5S"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "mi3CW9SPzLar"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents"
      ],
      "metadata": {
        "id": "J9ubY7OJzNVC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFD68ynazNnS",
        "outputId": "fdc02957-a924-4b18-ab70-8c992c9f3d79"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}